# EDA (Exploratory Data Analysis)

## 1. Perform Stastical Analysis

- Find out the no of features (numerical and categorical)

- Find out the statstical values of the features like mean(), range (min - max), 

## 2. Missing Value Analysis
- Plot missing value heatmap. (find out amount of missing data)

- Features with significant missing values (drop these features since they will have no significant impact on output)

- Features with moderate or single missing values 
    - impute them with mean() / median() for numerical features. 
    - impute them with mode() for categorical features


## 3. Outlier Analysis
Outliers in data causes skewness, and results in bias in the model. Hence, the outlier must be idenfied, and treated. Here are some techniques that could be used.

### **Univariate Analysis**
It is about studying central tendency and skewness of one feature at a time (could be input feature or output feature). It is performed on both numerical and categorical data. Objective is to normalize/linearize the data (remove the skewness) by using various scaling techniques (like log transformation, z-score standard scaling, frequency scaling for numerical data. and label-encoding or one-hot encoding for categorical data.)

### **Bivariate Analysis**
It is about finding co-relation between 2 features, and understand the cause-effect of one on another. You could find that by plotting:
- Numerical vs Numerical (scatter plot)
- Numerical vs Categorical (Box plot or ANOVA)
- Categorical vs Categorical (Box Plot, Mosaic Plot or Chi-square)

It is typically started with comparing each feature with TARGET-feature, and find its impact on the output.

### **Multivariate Analysis**
It is about finding **how all the features relate to the target and maybe even to each other.** 

Let's say you are predicting a house price based on input features Size, No_of_rooms, No_of_bathrooms, age_of_house. In this case, we try to answer the followings:

    - Does Size affect Price?

    - Does age_of_house affect price?

    - Do Bathrooms or Rooms matter?

    - Do combinations of features have an effect? (e.g., Size and Age together?)

Tools used are Correlation matrix and Pair plots. 

**Example Outcomes**

    - Size and Price have strong positive correlation → Bigger houses cost more.

    - Age and Price have negative correlation → Older houses are cheaper.

    - Bathrooms slightly boost price.

    - Size and Rooms are highly correlated.  (**this could be a problem**)


### **Multi-colinearity**
To detect whether two or more input features in your regression model are highly correlated, and if that correlation is hurting model performance. E.g. Size and Rooms are too simillar. This confuses the model.

    - Plot a heatmap of pairwise corelationships between features.

    - Any feature with a |co-relation| > 0.8 is a RED flag.

    - Calculate VIF to determine which one has higher VIF and value more than 5.

    $$
    \text{VIF}_i = \frac{1}{1 - R_i^2}
    $$

    Where:
    - $\text{VIF}_i$ is the Variance Inflation Factor for the $i^{th}$ feature
    - $R_i^2$ is the coefficient of determination from regressing the $i^{th}$ feature on all other independent features

    **Interpretation:**

    | VIF Value | Meaning                                |
    |-----------|-----------------------------------------|
    | 1         | No multicollinearity                    |
    | 1–5       | Moderate multicollinearity (acceptable) |
    | > 5       | High multicollinearity (check closely)  |
    | > 10      | Very high multicollinearity (problematic) |

    If the VIF of a feature is more than 10, remove the feature from the feature list.

## 4. Scaling - Feature Engineering
Scaling is essential in many machine learning and statistical models — especially when your dataset includes features with different units or ranges (like "age in years" vs. "price in dollars").

Example, In a house pricing dataset, the age could 1-2 digit number where as the price could be in 5-6 digits. The magnitude of the 2 features are very different. 

    - The distance-based Algorithms like Linear Regression, KNN, and Gradient Descent-based models are sensitive to magnitude.

    - Without scaling, the model will think "price" is more important than "age", just because the numbers are bigger.
    
    - Also when the feature has wide range. Example, sqft ranging from 100 to 1,000,000. In such case, the machine learning may endup with overfitting the model. Hence, we must linearize the data also.

Hence, scaling of feature is important. Here are some common scaling techniques:

    - Normaliztion (Min-Max Scaling): used for numeric data

    - Standardization (z-score scaling): used for numeric data

    - Log Transformation: used for numeric data

    - Label Encoding: used for numeric data

    - One-Hot Encoding: used for numeric data

