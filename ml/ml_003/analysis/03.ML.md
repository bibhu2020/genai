# Machine Learning

## Bias and Variance
**Bias** is the amount of assumptions your model has. It results in model **Underfitting** with the training data points. E.g. trying to fit a linear-regression algorithm to non-linear data. **_You notice underfitting when the model predicts poorly on both train and test data_**. Fixing **Underfitting** needs more complext model, feature engineering or longer training. 

**Variance** is the sensitivity of the model to the training data. More the sensitivity is, higher the variance will be. A small change in the data could result in high variance in the output. The model like this is called **Overfitting**. **_You notice this when the accuracy on train data is high and the accuracy on the trained data is relatively much lower_**. Either the model is overtrained, or a complext model is chosen which tries to fit into all the training data points. It memorizes each training data points. Hence, it may predict well with training data, but not with test or new data.  Fixing **overfitting** requires hyper-parameter tunning, cross-validation, ensemble learning, or trying different algorithm.

## Supervised Learning

### Regression
It is used where the dependant feature is a continous numerical value. E.g. predicted housing price.

#### Algorithms

- **Linear Regression**:
In the machine learning context, the Residual Error = (Y-actual) - (Y-predicted). The machine learning may predict multiple linear-line or model. 

- **Decission Tree**:
Tree-based algorithms are a class of machine learning models that use decision trees as their core building blocks to make predictions. They are widely used for both **classification** and **regression tasks**.

    - It is used when the features are both numeric and categorical

    - when the data is not-linear

    - Decission tree is easy to interpret and has high accuracy.

In decission tree, it is important to determine which feature should be the root, and which feature should follow after. Determining the feature is called **Attribute Selection Measures (ASM)**. ASM uses following techniques:

    - Information Gain

    - Gini Index


- **Random Forest**:

#### Model Evaluation    
In regression problems, the goal is to predict a continuous numerical value (e.g., price, temperature, score). The following metrics help evaluate the accuracy of regression models:

   **_Error Metrics_** measure how far off your predictions are from the actual values. Lower values mean better performance. Metrics are Mean Absolute Errors (MAE), Mean Squared Errors (MSE), Root Mean Square Error (RMSE), Sum of Squared Errors (SSE).


   **_Accuracy Metrics_** tell you how well your model is performing overall — in other words, how close the predictions are to the true values, either in percentage or in explained variance. Metrics are R² - Coefficient of Determination, Regression Accuracy.

   ```python
    print(model.coef_) #slop value or m value in y = mx + c
    print(model.intercept_) #y-intercep or c value in y = mx + c
   ```

### Classification
It is used when the Dependent variable is a categorical finite set of value. E.g Email is spam or pam.

#### Algorithms

- **Logistic Regression**:
Logistic Regression is a supervised machine learning algorithm used for classification problems — when your target variable is categorical (e.g., Yes/No, 0/1, Spam/Not Spam). The logistic regression model predicts the probability that the output y is 1 (e.g., "Yes", "Spam", "Not Spam") given the input features x.


- **Decission Tree**:


- **Random Forest**:


- **Ensemble Learning**:

#### Model Evaluation    
If the model is a classification model, you must use Precision, Recall or F1-Score for the model evaluation. 


## Unsupervised Learning

### Clustering


### Association Rule Learning


### Dimensionality Reduction


### Anomaly Detection


### Association Rule Learning


### Association Rule Learning


## Addressing Overfitting

### Hyper-parameter Tunning
It is about controlling the model training and strucutre. 