{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e938e94",
   "metadata": {},
   "source": [
    "# üìä Evaluating Large Language Models (LLMs)\n",
    "\n",
    "Assessing the performance of Large Language Models (LLMs) is crucial, but the right metrics depend on the task. This guide breaks down the key evaluation methods for two common NLP tasks: **classification** and **summarization**.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Evaluating Classification Tasks\n",
    "\n",
    "In **classification**, the goal is to assign the correct label to a piece of text. Performance is measured by how accurately the model makes this assignment.\n",
    "\n",
    "### The Confusion Matrix\n",
    "\n",
    "A **confusion matrix** is the foundation for most classification metrics. It visualizes the performance of a model by comparing predicted labels to the actual labels.\n",
    "\n",
    "|                    | **Predicted: Positive** | **Predicted: Negative** |\n",
    "| ------------------ | ----------------------- | ----------------------- |\n",
    "| **Actual: Positive** | ‚úÖ True Positive (TP)   | ‚ùå False Negative (FN)  |\n",
    "| **Actual: Negative** | ‚ùå False Positive (FP)  | ‚úÖ True Negative (TN)   |\n",
    "\n",
    "-   **True Positive (TP):** Correctly predicted positive.\n",
    "-   **False Positive (FP):** Incorrectly predicted positive (a \"false alarm\").\n",
    "-   **False Negative (FN):** Incorrectly predicted negative (a \"miss\").\n",
    "-   **True Negative (TN):** Correctly predicted negative.\n",
    "\n",
    "### Key Classification Metrics\n",
    "\n",
    "-   #### **Precision**\n",
    "    *\"Of all the positive predictions I made, how many were actually correct?\"*\n",
    "    -   **Formula:** `Precision = TP / (TP + FP)`\n",
    "    -   **Use Case:** Important when the cost of a false positive is high (e.g., spam detection).\n",
    "\n",
    "-   #### **Recall (Sensitivity)**\n",
    "    *\"Of all the actual positive cases, how many did I find?\"*\n",
    "    -   **Formula:** `Recall = TP / (TP + FN)`\n",
    "    -   **Use Case:** Important when the cost of a false negative is high (e.g., medical diagnosis).\n",
    "\n",
    "-   #### **Accuracy**\n",
    "    *\"Overall, what fraction of my predictions were correct?\"*\n",
    "    -   **Formula:** `Accuracy = (TP + TN) / (TP + TN + FP + FN)`\n",
    "    -   **Use Case:** A good general measure, but can be misleading on imbalanced datasets.\n",
    "\n",
    "-   #### **F1 Score**\n",
    "    *A balanced measure of precision and recall.*\n",
    "    -   **Formula:** `F1 = 2 * (Precision * Recall) / (Precision + Recall)`\n",
    "    -   **Use Case:** The best choice when you need to balance the concerns of both precision and recall, especially with uneven class distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úçÔ∏è Evaluating Summarization Tasks\n",
    "\n",
    "Summarization evaluation is more complex because there can be many \"correct\" summaries. The metrics depend on whether the summary is **extractive** or **abstractive**.\n",
    "\n",
    "### 1. Extractive Summarization\n",
    "\n",
    "In **extractive summarization**, the model selects and combines key sentences directly from the source text.\n",
    "\n",
    "-   #### **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\n",
    "    ROUGE measures the overlap of n-grams (sequences of words) between the model-generated summary and a human-written reference summary.\n",
    "    -   **ROUGE-1:** Measures the overlap of individual words (unigrams).\n",
    "    -   **ROUGE-2:** Measures the overlap of pairs of words (bigrams).\n",
    "    -   **ROUGE-L:** Measures the longest common subsequence of words, which accounts for sentence structure.\n",
    "\n",
    "### 2. Abstractive Summarization\n",
    "\n",
    "In **abstractive summarization**, the model generates new sentences to paraphrase the original content. This requires more sophisticated metrics that can understand semantic meaning.\n",
    "\n",
    "-   #### **BERTScore**\n",
    "    BERTScore goes beyond simple word overlap. It uses contextual embeddings from a pre-trained BERT model to compare the **semantic similarity** between the generated summary and the reference summary.\n",
    "    -   **How it works:** It computes the cosine similarity between the vector representations of words in both summaries, providing a much more nuanced measure of quality than ROUGE.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä At-a-Glance Summary\n",
    "\n",
    "| Metric      | Task Type        | What It Measures                                      |\n",
    "| ----------- | ---------------- | ----------------------------------------------------- |\n",
    "| **Precision** | Classification   | Correctness of positive predictions                   |\n",
    "| **Recall**    | Classification   | Ability to find all actual positive instances         |\n",
    "| **Accuracy**  | Classification   | Overall correctness of all predictions                |\n",
    "| **F1 Score**  | Classification   | The harmonic mean (balance) of Precision and Recall   |\n",
    "| **ROUGE**     | Summarization    | N-gram overlap between generated and reference summaries (extractive) |\n",
    "| **BERTScore** | Summarization    | Semantic similarity between summaries (abstractive)   |\n",
    "\n",
    "---\n",
    "\n",
    "## üèÅ Conclusion\n",
    "\n",
    "Evaluating LLMs effectively requires choosing the right tool for the job. For **classification**, metrics derived from the confusion matrix‚Äî**precision, recall, accuracy, and F1 score**‚Äîprovide a clear picture of a model's performance. For **summarization**, **ROUGE** is the standard for extractive tasks, while **BERTScore** offers a more semantically-aware evaluation for abstractive tasks. By using these metrics correctly, you can better understand, compare, and improve your LLM-powered applications."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
