# Langchain
LangChain is a framework for developing GenAI applications powered by large language models (LLMs). It provides the ecosystem to develop, evaulate, monitor GenAI application using any LLM. It is LLM agnostic.

## Langchain Packages
- langchain-core: It contains the base abstractions of different components. The interfaces for core components like LLMs, vector stores, retrievers and more are defined here. No third-party integrations in it.

- langchain: Contains chains, agents, and retrical strategies that make up and application's cognitive architecture. No third-party integrations.

- langchain-community: Contains third-party integrations that are maintained by Langchain Community.

While there is long trail of integrations in langchain-community, there are some popular integrations into their own packages. e.g. langchain-openai, and langchain-anthropic, etc...

- langchain-openai: Contains classes to connect with OpenAI and Azure Open AI.

- langgraph: is an extension of langchain aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph. It exposes high level interfaces for creating common type of agents as well as a low-level API for composing custom flows.

- langserve: A package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.

- langsmith: a developer platform that lets you debug, test, evaluate and monitor LLM applications. (using my google login)

## LLMChain vs ConversationalRetrievalChain vs LangChain Expression Language (LCEL) 

### LLMChain
LLMChain is the most basic and widely used chain in Langchain. It consists of prompt template and a language model.
`PromptTemplate + LLM = LLMChain`

**Use Cases**
- Simple prompt-based interactions with an LLM.
- Static question answering.
- Template-driven text generation.

Example:
```python
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.llms import OpenAI

prompt = PromptTemplate(input_variables=["product"], template="What is a good name for a {product}?")
llm_chain = LLMChain(prompt=prompt, llm=OpenAI())
llm_chain.run("smartwatch")
```

### ConversationalRetrievalChain 
ConversationalRetrievalChain is used to **chain LLM with document store (e.g. vector store, cache)**. It's used to build RAG application.

```ts
Chat history + Question → Rephrased Query
Rephrased Query + Vector DB → Relevant Documents
→ Combine context + Question → LLM
```

It helps in creating embedding the query, and sends the "retrieved documents + query" to llm to get the answer structured.

**Use Cases**
- Chatbots that need to access external knowledge (e.g., PDFs, databases).
- Q&A over long documents.
- Conversational memory with RAG (Retrieval-Augmented Generation).

Example:
```python
from langchain.chains import ConversationalRetrievalChain
from langchain.llms import OpenAI
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings

# Initialize LLM and document retrieval setup (use embedding model)
llm = OpenAI(model="text-davinci-003")
vectorstore = FAISS.load_local("document_store", OpenAIEmbeddings())

# Create the ConversationalRetrievalChain
convo_chain = ConversationalRetrievalChain.from_llm(llm, vectorstore)

# Run the conversational chain with a query
query = "What is the process to reset my password?"
response = convo_chain.run(query)
print(response)
```

### LangChain Expression Language (LCEL)
LCEL is a language designed for writing flexible, dynamic expressions and logic within the LangChain framework. It is used for creating complex, context-sensitive, and adaptable behaviors when interacting with models, data sources, or tools in LangChain.

`(prompt | retriever | llm | output_parser)`

LCEL is used to define expressions that determine which action to take based on dynamic conditions:

Example:
```python
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.schema.runnable import RunnableMap

prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
chain = prompt | ChatOpenAI()

chain.invoke({"topic": "cats"})
```
