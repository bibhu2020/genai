{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7974a333",
   "metadata": {},
   "source": [
    "## Build a GenAI Application\n",
    "This is the continuation of the part1. In Part1, we persisted the embeddings in a vectordB in the local disk. In the Part2, we will query the embeddings using llm, and chains.\n",
    "\n",
    "```\n",
    "User Query\n",
    "   ↓\n",
    "Retriever Chain → Documents\n",
    "   ↓\n",
    "Document Chain → LLM Answer\n",
    "\n",
    "```\n",
    "\n",
    "We will learn 2 chains here:\n",
    "\n",
    "1. Retriever Chain: A Retriever Chain is responsible for getting relevant documents from a knowledge base (vector store, keyword search, etc.) in response to a user query. \n",
    "\n",
    "```\n",
    "User Query → Retriever → Relevant Documents\n",
    "```\n",
    "\n",
    "2. Document Chain: A Document Chain (aka QA chain or Answer-generating chain) takes the documents retrieved (e.g., from a Retriever) and uses an LLM to synthesize an answer.\n",
    "\n",
    "```\n",
    "Query + Retrieved Docs → LLM → Final Answer\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b799ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Environment Variables from .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3729b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step0: Initialize the LLM Model\n",
    "from utility.llm_factory import LLMFactory\n",
    "llm = LLMFactory.get_llm('openai')\n",
    "\n",
    "from utility.embedding_factory import EmbeddingFactory\n",
    "\n",
    "embedding_model = EmbeddingFactory.get_llm('openai')\n",
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5af2cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step1: Load the vector store from the persisted directory\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "## Step 1: Load the vector store from the persisted directory\n",
    "vector_store_db_loaded = Chroma(    \n",
    "    collection_name=\"ragtest1_collection\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=\"./_data/chroma_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d127e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step2: Create a retriever from the vector store\n",
    "retriever = vector_store_db_loaded.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d51912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: Create a Dcoument Chain for Question Answering \n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the question based on the provided context:\n",
    "    <context>{context}</context>\n",
    "    Question: {input}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# link the LLM and the prompt to create a document chain\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    document_variable_name=\"context\"\n",
    ")\n",
    "\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3864c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: Create the Retriever Chain (by linking the retriever and document chain)\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever_chain = create_retrieval_chain(retriever, document_chain)\n",
    "print(retriever_chain)\n",
    "\n",
    "result = retriever_chain.invoke({\n",
    "    \"input\": \"How to find my current usages?\",\n",
    "    \"chat_history\": []\n",
    "})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1cca0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To change the organization level retention defaults, navigate to the Usage configuration tab and modify the settings. Bear in mind, this affects all new projects. For changing project level retention defaults, primarily for pre-existing projects, go to \"Projects\", select your project name, click the data retention drop-down, and modify it to base retention. This will affect retention and pricing for traces going forward.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's Query the retriever chain with a question\n",
    "# response = retriever_chain.invoke({\"input\":\"How to find my current usages?\"})\n",
    "response = retriever_chain.invoke({\"input\":\"How to change the retention defaults?\"})\n",
    "response[\"answer\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
